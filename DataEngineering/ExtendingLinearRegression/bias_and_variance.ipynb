{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro Discussion\n",
    "\n",
    "- What is our goal when creating a prediction model?\n",
    "- How do we measure that goal?\n",
    "- Are thre cases when our measurement of that goal conflicts with creating our model?\n",
    "- How does model complexity relate to all of this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recalling Training vs Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/learn-co-students/dsc-2-24-07-bias-variance-trade-off-online-ds-sp-000/raw/master/index_files/index_7_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/learn-co-students/dsc-2-24-07-bias-variance-trade-off-online-ds-sp-000/raw/master/index_files/index_11_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's fit with some fancy polynomial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/learn-co-students/dsc-2-24-07-bias-variance-trade-off-online-ds-sp-000/raw/master/index_files/index_14_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we succeed?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Variance & Bias\n",
    "\n",
    "$ MSE = Bias(\\hat{y})^2 + Var(\\hat{y}) + \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "When you're *off* from the bullseye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When would this occur?\n",
    "<!-- \n",
    "    * Wrong assumptions == model didn't catch all the use cases \n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "When you have bigger changes from each throw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When would this occur?\n",
    "<!-- \n",
    "    * Random noise leads to setting in the training on a sensitive model\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which fits with what (bias & variance)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![From https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229](https://cdn-images-1.medium.com/max/1600/1*RQ6ICt_FBSx6mkAsGVwx8g.png)\n",
    "\n",
    "> From: https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"shrink down\" prediction variables effects instead of deleting/zeroing them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Previously Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression - L2 Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a penalty ***hyperparameter*** $\\lambda$ for extra terms (large $m$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p m_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used mostly to prevent overfitting but since includes all features it can be computationally expensive (for many variables)\n",
    "\n",
    "Correlated values spread evenly on coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression - L1 Norm Regularization\n",
    "\n",
    "\"Least Absolute Shrinkage and Selection Operator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful since absolute value can be set at zero: performs estimation & selection (good for many variables) --> ***sparse solution***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
